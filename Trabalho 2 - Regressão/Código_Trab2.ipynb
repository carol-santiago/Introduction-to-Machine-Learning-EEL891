{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código Trabalho 2 (Regressão)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo os DataFrames de treinamento e de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('Dataset/conjunto_de_treinamento.csv')\n",
    "df_test = pd.read_csv('Dataset/conjunto_de_teste.csv')\n",
    "\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções usadas para \"tratar\" os dados: \n",
    "- Atribuir valores numéricos às variáveis nominais;\n",
    "- Remover variáveis que não trazem boas contribuições para o resultado final (olhar a sessão de \"análise de dados\" ao final do código)\n",
    "- Preenche espaços em branco (`null`) nas colunas que precisam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataProcessing1(df):\n",
    "    # Exclui colunas que não serão utilizadas\n",
    "    df.drop(columns=['Id', 'diferenciais', 'tipo_vendedor', 'area_extra', 'estacionamento'], inplace=True)\n",
    "    df.drop(columns=['quartos', 'piscina', 's_ginastica'], inplace=True)\n",
    "\n",
    "    # Transforma coluna 'tipo' em numérica\n",
    "    df['tipo'] = df['tipo'].map({'Apartamento': 1, 'Casa': 2})\n",
    "    df.fillna(0.5, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na função `DataProcessing1`, as variáveis `'tipo_vendedor'`, `'area_extra'`, `'estacionamento'` foram removidas por ter uma correlação muito baixa com as outras variáveis. Os gráficos a seguir mostram a correlação entre as variáveis para cada uma destas:\n",
    "\n",
    "![vendedor](C:\\Users\\carol\\OneDrive\\Documentos\\GitHub\\Introduction-to-Machine-Learning-EEL891\\Trabalho 2 - Regressão\\graficos correlação (variáveis excuídas)\\tipo_vendedor.png)\n",
    "\n",
    "![areaextra](C:\\Users\\carol\\OneDrive\\Documentos\\GitHub\\Introduction-to-Machine-Learning-EEL891\\Trabalho 2 - Regressão\\graficos correlação (variáveis excuídas)\\area_extra.png)\n",
    "\n",
    "![estacionamento](C:\\Users\\carol\\OneDrive\\Documentos\\GitHub\\Introduction-to-Machine-Learning-EEL891\\Trabalho 2 - Regressão\\graficos correlação (variáveis excuídas)\\estacionamento.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataProcessing2(df):\n",
    "    # Calcula a média dos valores dos imóveis por bairro\n",
    "    mean_encoded = df.groupby('bairro')['preco'].mean()\n",
    "    \n",
    "    # Cria um dicionário com os bairros originais e suas respectivas versões codificadas (valores médios dos imóveis)\n",
    "    bairros_config = mean_encoded.to_dict()\n",
    "    \n",
    "    # Mapeia os valores médios dos imóveis por bairro para todos os bairros\n",
    "    df['bairro'] = df['bairro'].map(mean_encoded)\n",
    "\n",
    "    return df, bairros_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CodificarBairroTeste(df, bairros_config):\n",
    "    # Codifica a coluna 'bairro' do conjunto de teste de acordo com o dicionário criado por DataProcessing2(df_train)\n",
    "    df['bairro'] = df['bairro'].map(bairros_config)\n",
    "    df_test['bairro'].fillna(df_test['bairro'].mean(), inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passa os DataFrames pelas funções de pré-processamento\n",
    "df_train = DataProcessing1(df_train)\n",
    "df_test = DataProcessing1(df_test)\n",
    "\n",
    "df_train, bairros_enc = DataProcessing2(df_train)\n",
    "df_test = CodificarBairroTeste(df_test, bairros_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checando o pré-processamento\n",
    "Verifica se todas as colunas dos DataFrames de treinamento e teste estão completamente preenchidas, e se o formato dos dados é numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta o número de valores NaN em cada coluna\n",
    "contagem_nan = df_test.isnull().sum(), df_train.isnull().sum()\n",
    "# Exibe a contagem de valores NaN\n",
    "print(contagem_nan[0], '\\n\\n', contagem_nan[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output desejado:\n",
    "\n",
    "df_test\n",
    "- tipo             0\n",
    "- bairro           0\n",
    "- suites           0\n",
    "- vagas            0\n",
    "- area_util        0\n",
    "- churrasqueira    0\n",
    "- playground       0\n",
    "- quadra           0\n",
    "- s_festas         0\n",
    "- s_jogos          0\n",
    "- sauna            0\n",
    "- vista_mar        0\n",
    "\n",
    "df_train\n",
    "- tipo             0\n",
    "- bairro           0\n",
    "- suites           0\n",
    "- vagas            0\n",
    "- area_util        0\n",
    "- churrasqueira    0\n",
    "- playground       0\n",
    "- quadra           0\n",
    "- s_festas         0\n",
    "- s_jogos          0\n",
    "- sauna            0\n",
    "- vista_mar        0\n",
    "- preco            0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do Modelo\n",
    "\n",
    "O `HistGradientBoostingRegressor` é um modelo de regressão baseado no algoritmo Gradient Boosting. A ideia principal por trás do Gradient Boosting é combinar vários estimadores fracos (geralmente árvores de decisão rasas) para formar um modelo mais poderoso. O `HistGradientBoostingRegressor`, especificamente, é uma implementação otimizada do Gradient Boosting que utiliza um algoritmo de aprendizado de histograma para melhorar a eficiência e o desempenho.\n",
    "\n",
    "O algoritmo de aprendizado de histograma divide os dados em intervalos discretos (histogramas) e opera diretamente nesses histogramas, em vez de usar valores individuais, tornando o processo de treinamento mais rápido e reduzindo a utilização de memória.\n",
    "\n",
    "Esse modelo é muito eficaz para problemas de regressão e é útil quando se deseja prever um valor numérico a partir de um conjunto de características.\n",
    "\n",
    "### Preparação dos Dados\n",
    "\n",
    "Antes de treinar o modelo, é necessário preparar os dados de treinamento e teste. Para isso, separamos as variáveis independentes (`X_train` e `X_test`) e a variável alvo (`y_train`) do conjunto de treinamento. \n",
    "\n",
    "### Criação e Treinamento do Modelo\n",
    "Após a preparação dos dados, é criada uma instância do regressor `HistGradientBoostingRegressor` com os parâmetros desejados para configurar o modelo. Neste exemplo, serão utilizados alguns parâmetros específicos:\n",
    "\n",
    "- `l2_regularization`: Um parâmetro de regularização L2 que controla o nível de regularização aplicado às funções de base. Valores maiores deste parâmetro aplicam mais regularização, ajudando a evitar overfitting (sobreajuste).\n",
    "\n",
    "- `max_iter`: O número máximo de iterações (etapas) do boosting, ou seja, o número máximo de árvores de decisão que serão construídas durante o processo de treinamento.\n",
    "\n",
    "- `loss`: A função de perda utilizada para medir a qualidade das previsões. Neste caso, utilizamos \"absolute_error\", que é a função de erro absoluto médio.\n",
    "\n",
    "- `max_depth`: A profundidade máxima das árvores de decisão usadas no modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Separa os dados de treino e teste\n",
    "X_train = df_train.iloc[:, :-1].values\n",
    "y_train = df_train.iloc[:, -1].values\n",
    "X_test = df_test.iloc[:, :].values\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# Cria e treina o modelo\n",
    "model = HistGradientBoostingRegressor(l2_regularization=34, max_iter=140, loss = \"absolute_error\", max_depth=12)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um DataFrame com a coluna 'Id' e 'preco' contendo os valores de y_pred\n",
    "df_resultado = pd.DataFrame({'Id': range(len(y_pred)), 'preco': y_pred})\n",
    "\n",
    "# Salva o DataFrame em um arquivo CSV\n",
    "df_resultado.to_csv('resultadofinalhist.csv', index=False)\n",
    "df_resultado.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de dados\n",
    "\n",
    "### Função plot_feature_importance\n",
    "\n",
    "A função `plot_feature_importance` tem o objetivo de calcular e visualizar a importância das características (recursos) do modelo `HistGradientBoostingRegressor`. Essa importância é uma medida que indica o impacto que cada característica tem no desempenho do modelo para fazer previsões.\n",
    "\n",
    "O cálculo da importância das características é feito usando a técnica de importância de permutação (`permutation_importance`). Essa técnica consiste em medir o efeito de embaralhar aleatoriamente os valores de uma característica e verificar como isso afeta o desempenho do modelo. Se a importância de uma característica for alta, significa que embaralhar seus valores causará uma redução significativa na precisão do modelo, indicando que a característica é importante para fazer previsões precisas.\n",
    "\n",
    "A função `plot_feature_importance` recebe os seguintes parâmetros:\n",
    "\n",
    "- `model`: O modelo treinado `HistGradientBoostingRegressor`.\n",
    "- `X`: A matriz de características (recursos) de treinamento.\n",
    "- `y`: O vetor de valores alvo (rótulos) correspondentes às amostras em `X`.\n",
    "- `feature_names`: Uma lista com os nomes das características.\n",
    "- A função utiliza a biblioteca permutation_importance do scikit-learn para calcular as importâncias das características;\n",
    "- Em seguida, ela organiza as características em ordem decrescente de importância e plota um gráfico de barras para visualizar a importância de cada característica.\n",
    "\n",
    "Além disso, a função também imprime uma lista `unimportant_features` que, como o nome sugere, contém as características que têm importância igual a zero, o que indica que essas características provavelmente não estão contribuindo para o desempenho do modelo e poderiam ser consideradas para remoção.\n",
    "\n",
    "Essa função , em específico sua propriedade de listar as features menos importantes, foi utilizada para identificar quais variáveis deveriam ser removidas. A partir da análise do gráfico de barras, foi possível identificar que as variáveis `'quartos'`, `'piscina'` e `'s_ginastica'` não contribuem para o desempenho do modelo e, por isso, foram removidas. A seguir, o gráfico de barras com a importância das características do modelo antes da remoção dessas variáveis:\n",
    "\n",
    "![Gráfico pré-remoção](C:\\Users\\carol\\OneDrive\\Documentos\\GitHub\\Introduction-to-Machine-Learning-EEL891\\Trabalho 2 - Regressão\\graficos correlação (variáveis excuídas)\\preremocaoquartos.png)\n",
    "\n",
    "Pós a remoção das variávies, o gráfico de barras ficou da seguinte forma:\n",
    "\n",
    "![Gráfico pré-remoção](C:\\Users\\carol\\OneDrive\\Documentos\\GitHub\\Introduction-to-Machine-Learning-EEL891\\Trabalho 2 - Regressão\\graficos correlação (variáveis excuídas)\\posremocao.png)\n",
    "\n",
    "A escala não permite tal percepção, porém, as variáveis `'quadra'` e `'s_jogos'` não têm importância nula segundo a função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def plot_feature_importance(model, X, y, feature_names):\n",
    "    # Define metodo de calcular importancias\n",
    "    result = permutation_importance(model, X, y, n_repeats=10, random_state=42)\n",
    "\n",
    "    # Calcula as importancias e suas incertezas\n",
    "    feature_importance = result.importances_mean\n",
    "    feature_importance_std = result.importances_std\n",
    "\n",
    "    # Cria um df com importancias e incertezas\n",
    "    feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance, 'Std Dev': feature_importance_std})\n",
    "    unimportant_features = []\n",
    "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    for i in range(len(feature_importance_df)):\n",
    "        if feature_importance_df['Importance'][i] <= 0:\n",
    "            unimportant_features.append(feature_importance_df['Feature'][i])\n",
    "\n",
    "    # Plota as importancias\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "    plt.errorbar(x=feature_importance_df['Importance'], y=feature_importance_df['Feature'],\n",
    "                 xerr=feature_importance_df['Std Dev'], fmt='o')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance in HistGradientBoostingRegressor')\n",
    "    plt.show()\n",
    "    \n",
    "    print(unimportant_features)\n",
    "\n",
    "    \n",
    "feature_names = df_train.columns[:-1]\n",
    "plot_feature_importance(model, X_train, y_train, feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trabalho-dois",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
